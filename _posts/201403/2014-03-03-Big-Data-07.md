---
layout:	post
title:	Big Data In Action 07  MapReduce WordCount
description: 大数据实战系列07 MapReduce WordCount
categories:
- Hadoop 
tags:
- MapReduce
---

###准备阶段:
1. 准备2个文本文件用以统计分析:
	
		#file01.txt
		hadoop
		hdfs
		mapreduce
		hello world
		flexie

		#file02.txt
		hadoop
		mapreduce
		hello
		flexie
		bigdata	
2. 利用Hadoop Java API上传至HDFS
		
		import org.fle4y.util.MyFileUtil;

		public class Main {

		public static void main(String[] args) throws Exception {
			String src="input/exam02/file01.txt";
			String dst="/input/exam02/file01.txt";
			MyFileUtil.createFromLocalFile(src, dst);
			src="input/exam02/file02.txt";
			dst="/input/exam02/file02.txt";
			MyFileUtil.createFromLocalFile(src, dst);
			}

		}


	![效果][1]
	


3. 编写MapReduce代码
	
		package org.fle4y.example02;
		import java.io.IOException;
		import org.apache.hadoop.conf.Configuration;
		import org.apache.hadoop.fs.Path;
		import org.apache.hadoop.io.IntWritable;
		import org.apache.hadoop.io.Text;
		import org.apache.hadoop.mapreduce.Job;
		import org.apache.hadoop.mapreduce.Mapper;
		import org.apache.hadoop.mapreduce.Reducer;
		import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
		import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
		import org.fle4y.util.ConfigurationFactory;

		/**
 		* @author: xiehui
 		* @mailto: flexie@foxmail.com
 		* @date: 2014年3月3日
 		* @Blog : http://0xc000005.github.io/
 		*  
 		*  MapReduce WordCount Example
 		*/
		public class WordCountExample {
		private static class WordCountMapper extends Mapper<Object,Text,Text,IntWritable>{
			@Override
			protected void map(Object key, Text value, Context context)
					throws IOException, InterruptedException {
				String input = value.toString();
				String strs[] = input.split(" ");
				for(String str : strs){
					context.write(new Text(str), new IntWritable(1));
				}
			}
			
		}
		
		private static class WordCountReduce extends Reducer<Text,IntWritable,Text,IntWritable>{
			
			@Override
			protected void reduce(Text key, Iterable<IntWritable> values,Context context)
					throws IOException, InterruptedException {
				int sum = 0;
				for(IntWritable num : values){
					sum +=num.get();
				}
				context.write(key, new IntWritable(sum));
			}
		}
		
		public static void main(String[] args) throws Exception{
				Configuration conf = ConfigurationFactory.createConfigure();
				Job job = Job.getInstance(conf);
				job.setJobName("Word Count");
				job.setJarByClass(WordCountExample.class);
				job.setMapperClass(WordCountMapper.class);
				job.setMapOutputKeyClass(Text.class);
				job.setMapOutputValueClass(IntWritable.class);
				job.setReducerClass(WordCountReduce.class);
				job.setOutputKeyClass(Text.class);
				job.setOutputValueClass(IntWritable.class);
				FileInputFormat.addInputPath(job, new Path("/input/exam02"));  
	        	FileOutputFormat.setOutputPath(job, new Path("/output/exam02"));  
	        	System.exit(job.waitForCompletion(true)?0:1);  
			}
		}


4. 运行效果：
	
	![效果][3]


5. 项目地址[下载][4]

###问题:
下载[hadoop-commin.zip][2],因为在windows下运行，压缩包里面缺少 winutils.exe, hadoop.dll等文件，下载完成后，将要报下面的bin目录下的所有文件全部拷贝到hadoop目录下的bin文件夹下
###常用链接

[![给我捐款](http://c000005.qiniudn.com/donate_me.png "给我捐款")](http://me.alipay.com/0xc000005)

[回到主页][5]

                                               


[1]: http://c000005.qiniudn.com/6.PNG
[2]: https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip
[3]: http://c000005.qiniudn.com/7.PNG
[4]: https://github.com/0xC000005/Hadoop
[5]: http://0xc000005.github.io/