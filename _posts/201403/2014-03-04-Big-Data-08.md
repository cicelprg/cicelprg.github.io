---
layout:	post
title:	Big Data In Action 08  分布式环境配置
description: 大数据实战系列08 分布式环境配置
categories:
- Hadoop 
tags:
- Hadoop
---
###准备阶段
*先参考虚拟环境伪分布式单机Hadoop配置，把JDK等环境变量配置好，然后进行下一步.*
[伪分布式环境搭建][7]
###分布式配置:
1. 修改vagrant配置，增加多台虚拟机
	
	
	  	config.vm.define :cloud01 do |cloud01|
    		cloud01.vm.provider "virtualbox" do |v|
          		v.customize ["modifyvm", :id, "--name", "cloud01", "--memory", "512"]
    		end
    		cloud01.vm.box = "base"
    		cloud01.vm.hostname = "cloud01"
    		cloud01.vm.network :private_network, ip: "11.11.1.100"
 		end

		config.vm.define :cloud02 do |cloud02|
    	cloud02.vm.provider "virtualbox" do |v|
          		v.customize ["modifyvm", :id, "--name", "cloud02", "--memory", "512"]
    		end
    	cloud02.vm.box = "base"
    	cloud02.vm.hostname = "cloud02"
    	cloud02.vm.network :private_network, ip: "11.11.1.200"
  		end
  
  		config.vm.define :cloud03 do |cloud03|
    		cloud03.vm.provider "virtualbox" do |v|
          		v.customize ["modifyvm", :id, "--name", "cloud03", "--memory", "512"]
    		end
    	cloud03.vm.box = "base"
    	cloud03.vm.hostname = "cloud03"
    	cloud03.vm.network :private_network, ip: "11.11.1.201"
  		end

2. 集群分配策略

		11.11.1.100     cloud01		master      namenode/secondnamenode/resourcemanager     

		11.11.1.200    	cloud02 	slave       datanode/nodemanager

		11.11.1.201     cloud03 	slave       datanode/nodemanager              

3. 分别修改cloud01 cloud02 cloud03 的hosts文件：
		
		sudo vim /etc/hosts
   		修改为:
  		11.11.1.100 cloud01 cloud01
		11.11.1.200 cloud02 cloud02
 		11.11.1.201 cloud03 cloud03

4. 修改core-site.xml
		
		 
		<configuration>
    	<property>
        <name>fs.default.name</name>
        <value>hdfs://cloud01:8020</value>
    	</property>
     	<property>
        <name>hadoop.tmp.dir</name>
        <value>/home/vagrant/hadoop/data/tmp</value>
    	</property>
   	 	<property>
        <name>fs.checkpoint.period</name>
        <value>300</value>
     	</property>
    	<property>
        <name>fs.checkpoint.dir</name>
        <value>${hadoop.tmp.dir}/dfs/namesecondary</value>
     	</property>
		</configuration>   
5. 修改hdfs-site.xml

		<configuration>
    		<property>
        		<name>dfs.replication</name>
        		<value>2</value>
    		</property>
    		<property>
        		<name>dfs.namenode.name.dir</name>
        		<value>file:/home/vagrant/hadoop/data/hdfs/namenode</value>
    		</property>
    		<property>
        		<name>dfs.datanode.data.dir</name>
        		<value>file:/home/vagrant/hadoop/data/hdfs/datanode</value>
    		</property>
    		<property>
        		<name>dfs.http.address</name>
       			<value>0.0.0.0:50070</value>
        	</property>
    		<property>
        		<name>dfs.datanode.http.address</name>
        		<value>0.0.0.0:50075</value>
    		</property>
    		<property>
        		<name>dfs.permissions</name>
        		<value>false</value>
    		</property>
		</configuration>  
 
6.   配置masters和slaves主从结点 

     配置conf/slaves来设置主从结点，注意最好使用主机名，并且保证机器之间通过主机名可以互相访问，每个主机名一行。 

	vi slaves： 

	输入： 

	cloud02

	cloud03

7.	启动集群:
	
	只需要在Cloud01 机器执行:
	
		hadoop namenode -format
		start-dfs.sh 
		start-yarn.sh

8.	观察集群:
		
	![cloud01][1]

	![cloud02][2]

	![cloud03][3]

8. 在集群上运行WordCount：

	![wordcount][6]

###常用链接

**捧个钱场**

[![给我捐款](http://c000005.qiniudn.com/donate_me.png "给我捐款")](http://me.alipay.com/0xc000005)

[回到主页][5]

                                               


[1]: http://c000005.qiniudn.com/8.PNG
[2]: http://c000005.qiniudn.com/9.PNG
[3]: http://c000005.qiniudn.com/10.PNG
[6]: http://c000005.qiniudn.com/11.PNG
[4]: https://github.com/0xC000005/Hadoop
[5]: http://0xc000005.github.io/
[7]: http://0xc000005.github.io/2014/01/Big-Data-03/